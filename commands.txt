# DATA FILES
mkdir -p data/metabase && \
mkdir -p data/mongodb && \
mkdir -p data/zookeeper1/data && \
mkdir -p data/zookeeper1/log && \
mkdir -p data/zookeeper2/data && \
mkdir -p data/zookeeper2/log && \
mkdir -p data/zookeeper3/data && \
mkdir -p data/zookeeper3/log && \
mkdir -p data/broker1/data && \
mkdir -p data/broker2/data && \
mkdir -p data/broker3/data && \
sudo chown -R 1000:1000 data



# DOCKER
docker compose up -d
docker compose down
docker compose ps
docker compose logs -f
docker exec -it broker1 bash
docker exec broker2 kafka-server-stop
docker exec zookeeper2 zookeeper-server-stop
sudo service docker restart
docker-compose restart broker1
docker-compose stop producer-service
docker-compose rm kafka-connect
docker-compose up -d kafka-connect
docker-compose up -d zookeeper1 zookeeper2 zookeeper3 broker1 broker2 broker3 kafdrop mongodb metabase kafka-connect 
docker-compose up -d producer-service processor-service
docker-compose rm -svf



# TOPICS KAFKA
kafka-topics --bootstrap-server broker1:9092 --create --topic data_received --partitions 1 --replication-factor 3 --if-not-exists --config min.insync.replicas=2

kafka-topics --bootstrap-server broker1:9092 --create --topic data_processed --partitions 1 --replication-factor 3 --if-not-exists --config retention.ms=-1 --config min.insync.replicas=2

kafka-topics --bootstrap-server broker1:9092 --create --topic data_received_failed --partitions 1 --replication-factor 3 --if-not-exists --config retention.ms=-1 --config min.insync.replicas=2

kafka-topics --bootstrap-server broker1:9092 --topic data_processed --alter --partitions 3

kafka-topics --bootstrap-server broker1:9092 --topic data_received --describe

kafka-topics --bootstrap-server broker1:9092 --topic data_processed --delete

kafka-topics --bootstrap-server broker1:9092 --list


kafka-console-producer --bootstrap-server broker1:9092 --topic data_received
kafka-console-consumer --bootstrap-server broker1:9092 --topic data_processed --from-beginning




# RUN TESTS
kafka-console-consumer --bootstrap-server broker1:9092,broker2:9094,broker3:9096 --topic data_test
kafka-console-producer --bootstrap-server broker1:9092,broker2:9094,broker3:9096



# KAFKA CONSUMERS
kafka-consumer-groups --bootstrap-server broker1:9092 --describe --all-groups
kafka-consumer-groups --bootstrap-server broker1:9092 --delete-offsets --group processor-service-group --topic data_received
kafka-consumer-groups --bootstrap-server broker1:9092 --group processor-service-group \
  --topic data_received --reset-offsets --to-datetime '2021-10-07T00:00:01.111' --timeout 150000 --execute
kafka-consumer-groups --bootstrap-server broker1:9092 --group processor-service-group \
  --topic data_received:0 --reset-offsets --shift-by -3 --timeout 150000 --execute
kafka-consumer-groups --bootstrap-server broker1:9092 --group processor-service-group \
  --reset-offsets  --topic data_received:0 --to-earliest --execute
kafka-consumer-groups --bootstrap-server broker1:9092 --group processor-service-group \
  --reset-offsets  --topic data_received:0 --to-offset 63 --execut



# KAFKA CONNECTOR (connectors data are saved at kafka broker connectors topic)
curl -X POST -H "Content-Type: application/json" --data @connector.json http://localhost:8083/connectors
curl -X DELETE http://localhost:8083/connectors/mongodb-sink-connector
kafka-console-consumer --bootstrap-server broker1:9092 --topic connect-configs --from-beginning
kafka-console-consumer --bootstrap-server broker1:9092 --topic connect-status --from-beginning



# MONGO -- config via Compass interface (create a database with a collection)
# METABASE -- config via interface (create a conection with mongo)




# ZOOKEEPER
echo "srvr" | nc localhost 2183   # see zookeeper Mode (follower or leader)
