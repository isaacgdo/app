# DATA FILES
CURRENT_UID=$(id -u):$(id -g)
mkdir -p datakafka/zookeeper1/data && \
mkdir -p datakafka/zookeeper1/log && \
mkdir -p datakafka/zookeeper2/data && \
mkdir -p datakafka/zookeeper2/log && \
mkdir -p datakafka/zookeeper3/data && \
mkdir -p datakafka/zookeeper3/log && \
mkdir -p datakafka/broker1/data && \
mkdir -p datakafka/broker2/data && \
mkdir -p datakafka/broker3/data
sudo chown -R 1000:1000 datakafka



# DOCKER
docker compose up -d
docker compose down
docker compose ps
docker compose logs -f
docker exec -it broker1 bash
sudo service docker restart
docker-compose restart broker1
docker-compose stop producer-service
docker-compose rm kafka-connect
docker-compose up -d kafka-connect
docker-compose up -d zookeeper1 zookeeper2 zookeeper3 broker1 broker2 broker3 kafka-connect 
docker-compose up -d mongodb metabase
docker-compose up -d producer-service processor-service
docker-compose rm -svf



# TOPICS KAFKA
kafka-topics --bootstrap-server broker1:9092 --create --topic data_received --partitions 1 --if-not-exists \
 --config cleanup.policy=delete --config max.message.bytes=2097164

kafka-topics --bootstrap-server broker1:9092 --create --topic data_processed --partitions 1 --if-not-exists \
 --config cleanup.policy=delete --config retention.ms=-1 --config max.message.bytes=2097164

kafka-topics --bootstrap-server broker1:9092 --create --topic data_received_failed --partitions 1 --if-not-exists \
 --config cleanup.policy=delete --config retention.ms=-1 --config max.message.bytes=2097164

kafka-topics --bootstrap-server broker1:9092 --topic data_processed --alter --timeout 150000 --partitions 3

kafka-topics --bootstrap-server broker1:9092 --topic data_received --describe

kafka-topics --bootstrap-server broker1:9092 --topic data_received_faled --delete

kafka-topics --bootstrap-server broker1:9092 --list



kafka-console-producer --bootstrap-server broker1:9092 --topic data_received
kafka-console-consumer --bootstrap-server broker1:9092 --topic data_processed --from-beginning



# KAFKA CONSUMERS
kafka-consumer-groups --bootstrap-server broker1:9092 --describe --all-groups
kafka-consumer-groups --bootstrap-server broker1:9092 --delete-offsets --group processor-service-group --topic data_received
kafka-consumer-groups --bootstrap-server broker1:9092 --group processor-service-group \
  --topic data_received --reset-offsets --to-datetime '2021-10-07T00:00:01.111' --timeout 150000 --execute
kafka-consumer-groups --bootstrap-server broker1:9092 --group processor-service-group \
  --topic data_received:0 --reset-offsets --shift-by -3 --timeout 150000 --execute
kafka-consumer-groups --bootstrap-server broker1:9092 --group processor-service-group \
  --reset-offsets  --topic data_received:0 --to-earliest --execute
kafka-consumer-groups --bootstrap-server broker1:9092 --group processor-service-group \
  --reset-offsets  --topic data_received:0 --to-offset 63 --execut



# KAFKA CONNECTOR (os connectors criados ficam salvos nos tópicos do connect no broker do kafka)
curl -X POST -H "Content-Type: application/json" --data @connector.json http://localhost:8083/connectors
curl -X DELETE http://localhost:8083/connectors/mongodb-sink-connector
kafka-console-consumer --bootstrap-server broker1:9092 --topic connect-configs --from-beginning
kafka-console-consumer --bootstrap-server broker1:9092 --topic connect-status --from-beginning



# MONGO -- configuração via interface do Compass (cria database com collection)
# METABASE -- configuração via interface (conexão com o banco do mongo) (senha rn96is)